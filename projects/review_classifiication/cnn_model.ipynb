{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "#import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import os\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.metrics import categorical_accuracy, categorical_crossentropy\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 395772404296247133\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4945621811\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6484935577701039209\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.kaggle.com/carlosaguayo/deep-learning-for-text-classification#Loading-GloVe-embeddings\n",
    "http://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "max_sentence_len = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_level5.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lab = \"level5_clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"Review\"][0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3283"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data[\"Review\"].apply(lambda txt: len(txt.split(\" \"))>max_sentence_len)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517137"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3282 of 500k reviews have more than 300 words. Dropping them so we won't train on partial strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"Review\"].apply(lambda txt: len(txt.split(\" \"))<=max_sentence_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = data.Review # Extract text\n",
    "target = data[y_lab] # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78967"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513854"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513854"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i bought 3 products from this new mongongo line shampoo moisture seal masque and moisture styling gel it went on smoothly but when it dried my 2 4 c curly hair frizzed it s crispy and dry i wish i could take all of them back the other products in the old line work very well curl enhancing smoothie curling gel souffle and the deep treatment masque work good i was hoping for even more improvement with the new mongongo line not so beware you will have thick dry frizz "
     ]
    }
   ],
   "source": [
    "inv_index = {v:k for k,v in tokenizer.word_index.items()}\n",
    "\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.88358366384226, 45.26998350757918)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len = sum(map(len, sequences))/len(sequences)\n",
    "std_len = np.sqrt(sum(map(lambda x:(len(x) - avg_len) **2 , sequences))/len(sequences))\n",
    "\n",
    "avg_len, std_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513854, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function/performance    0.718801\n",
       "appearance              0.154945\n",
       "packaging/labeling      0.069181\n",
       "assembly/preparation    0.033654\n",
       "aroma/flavor            0.022339\n",
       "others                  0.001080\n",
       "Name: level5_clean, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (513854, 300)\n",
      "Shape of labels: (513854, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove-global-vectors-for-word-representation' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt'), \"rb\") as f:\n",
    "    for n,line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0].decode(\"utf-8\") # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_index2 = {}\n",
    "#for key_ in embeddings_index:\n",
    "#    embeddings_index2[key_.decode(\"utf-8\")] = embeddings_index[key_]\n",
    "#embeddings_index = embeddings_index2\n",
    "#del embeddings_index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(embeddings_index[\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 298, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 97, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,301,830\n",
      "Trainable params: 301,830\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_sentence_len, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False)) \n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 411083 samples, validate on 102771 samples\n",
      "Epoch 1/2\n",
      "411083/411083 [==============================] - 829s 2ms/step - loss: 0.6036 - categorical_accuracy: 0.7729 - val_loss: 0.5666 - val_categorical_accuracy: 0.7830\n",
      "Epoch 2/2\n",
      "113472/411083 [=======>......................] - ETA: 9:33 - loss: 0.5545 - categorical_accuracy: 0.7863"
     ]
    }
   ],
   "source": [
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])\n",
    "\n",
    "#model.fit(data, labels, validation_split=0.2, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411083, 300)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 328866 samples, validate on 82217 samples\n",
      "Epoch 1/10\n",
      "328866/328866 [==============================] - 80s 243us/step - loss: 0.6129 - categorical_accuracy: 0.7698 - val_loss: 0.5669 - val_categorical_accuracy: 0.7838\n",
      "Epoch 2/10\n",
      "328866/328866 [==============================] - 83s 252us/step - loss: 0.5554 - categorical_accuracy: 0.7864 - val_loss: 0.5611 - val_categorical_accuracy: 0.7864\n",
      "Epoch 3/10\n",
      "328866/328866 [==============================] - 77s 233us/step - loss: 0.5386 - categorical_accuracy: 0.7919 - val_loss: 0.5531 - val_categorical_accuracy: 0.7880\n",
      "Epoch 4/10\n",
      "328866/328866 [==============================] - 76s 232us/step - loss: 0.5275 - categorical_accuracy: 0.7955 - val_loss: 0.5576 - val_categorical_accuracy: 0.7889\n",
      "Epoch 5/10\n",
      "328866/328866 [==============================] - 77s 233us/step - loss: 0.5173 - categorical_accuracy: 0.7988 - val_loss: 0.5524 - val_categorical_accuracy: 0.7869\n",
      "Epoch 6/10\n",
      "328866/328866 [==============================] - 77s 234us/step - loss: 0.5086 - categorical_accuracy: 0.8012 - val_loss: 0.5581 - val_categorical_accuracy: 0.7851\n",
      "Epoch 7/10\n",
      "328866/328866 [==============================] - 77s 234us/step - loss: 0.5003 - categorical_accuracy: 0.8039 - val_loss: 0.5809 - val_categorical_accuracy: 0.7765\n",
      "Epoch 8/10\n",
      "328866/328866 [==============================] - 77s 235us/step - loss: 0.4931 - categorical_accuracy: 0.8061 - val_loss: 0.5768 - val_categorical_accuracy: 0.7870\n",
      "Epoch 9/10\n",
      "328866/328866 [==============================] - 77s 236us/step - loss: 0.4858 - categorical_accuracy: 0.8081 - val_loss: 0.5761 - val_categorical_accuracy: 0.7834\n",
      "Epoch 10/10\n",
      "328866/328866 [==============================] - 78s 237us/step - loss: 0.4797 - categorical_accuracy: 0.8110 - val_loss: 0.5941 - val_categorical_accuracy: 0.7843\n",
      "Wall time: 12min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[categorical_accuracy])\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['appearance', 'aroma/flavor', 'assembly/preparation',\n",
       "       'function/performance', 'others', 'packaging/labeling'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['function/performance', 'function/performance',\n",
       "       'packaging/labeling', 'function/performance', 'aroma/flavor'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(model.predict(X_test[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['function/performance', 'function/performance',\n",
       "       'packaging/labeling', 'function/performance', 'aroma/flavor'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred_classes = encoder.inverse_transform(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_classes =  encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854745015617246"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_classes, y_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          appearance       0.45      0.55      0.49     12902\n",
      "        aroma/flavor       0.46      0.63      0.54      1730\n",
      "assembly/preparation       0.50      0.58      0.54      2963\n",
      "function/performance       0.91      0.85      0.88     79347\n",
      "              others       0.01      0.33      0.02         3\n",
      "  packaging/labeling       0.46      0.56      0.50      5826\n",
      "\n",
      "           micro avg       0.79      0.79      0.79    102771\n",
      "           macro avg       0.46      0.58      0.49    102771\n",
      "        weighted avg       0.81      0.79      0.80    102771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred_classes, y_test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn_model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"cnn_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 298, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 97, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,301,830\n",
      "Trainable params: 301,830\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.get_layer(\"embedding_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [-0.038194, -0.24487 ,  0.72812 , ..., -0.1459  ,  0.8278  ,\n",
       "          0.27062 ],\n",
       "        [-0.046539,  0.61966 ,  0.56647 , ..., -0.37616 , -0.032502,\n",
       "          0.8062  ],\n",
       "        ...,\n",
       "        [ 0.49531 ,  0.11368 ,  0.096595, ..., -0.10373 , -1.9915  ,\n",
       "         -0.069738],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [-0.78288 ,  0.28184 , -0.71599 , ...,  0.455   ,  0.037052,\n",
       "         -0.23365 ]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m[\u001b[0m\u001b[1;34m'input_dim'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'output_dim'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embeddings_initializer='uniform'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embeddings_regularizer=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'activity_regularizer=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embeddings_constraint=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mask_zero=False'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'input_length=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'**kwargs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Turns positive integers (indexes) into dense vectors of fixed size.\n",
       "eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
       "\n",
       "This layer can only be used as the first layer in a model.\n",
       "\n",
       "# Example\n",
       "\n",
       "```python\n",
       "  model = Sequential()\n",
       "  model.add(Embedding(1000, 64, input_length=10))\n",
       "  # the model will take as input an integer matrix of size (batch, input_length).\n",
       "  # the largest integer (i.e. word index) in the input should be\n",
       "  # no larger than 999 (vocabulary size).\n",
       "  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
       "\n",
       "  input_array = np.random.randint(1000, size=(32, 10))\n",
       "\n",
       "  model.compile('rmsprop', 'mse')\n",
       "  output_array = model.predict(input_array)\n",
       "  assert output_array.shape == (32, 10, 64)\n",
       "```\n",
       "\n",
       "# Arguments\n",
       "    input_dim: int > 0. Size of the vocabulary,\n",
       "        i.e. maximum integer index + 1.\n",
       "    output_dim: int >= 0. Dimension of the dense embedding.\n",
       "    embeddings_initializer: Initializer for the `embeddings` matrix\n",
       "        (see [initializers](../initializers.md)).\n",
       "    embeddings_regularizer: Regularizer function applied to\n",
       "        the `embeddings` matrix\n",
       "        (see [regularizer](../regularizers.md)).\n",
       "    embeddings_constraint: Constraint function applied to\n",
       "        the `embeddings` matrix\n",
       "        (see [constraints](../constraints.md)).\n",
       "    mask_zero: Whether or not the input value 0 is a special \"padding\"\n",
       "        value that should be masked out.\n",
       "        This is useful when using [recurrent layers](recurrent.md)\n",
       "        which may take variable length input.\n",
       "        If this is `True` then all subsequent layers\n",
       "        in the model need to support masking or an exception will be raised.\n",
       "        If mask_zero is set to True, as a consequence, index 0 cannot be\n",
       "        used in the vocabulary (input_dim should equal size of\n",
       "        vocabulary + 1).\n",
       "    input_length: Length of input sequences, when it is constant.\n",
       "        This argument is required if you are going to connect\n",
       "        `Flatten` then `Dense` layers upstream\n",
       "        (without it, the shape of the dense outputs cannot be computed).\n",
       "\n",
       "# Input shape\n",
       "    2D tensor with shape: `(batch_size, sequence_length)`.\n",
       "\n",
       "# Output shape\n",
       "    3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n",
       "\n",
       "# References\n",
       "    - [A Theoretically Grounded Application of Dropout in\n",
       "       Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\dilee\\anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 298, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 97, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,301,830\n",
      "Trainable params: 301,830\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_sentence_len, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False)) \n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.3)) ## Adding Dropout\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411083, 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328866 samples, validate on 82217 samples\n",
      "Epoch 1/10\n",
      "328866/328866 [==============================] - 79s 240us/step - loss: 0.6240 - categorical_accuracy: 0.7690 - val_loss: 0.5726 - val_categorical_accuracy: 0.7825\n",
      "Epoch 2/10\n",
      "328866/328866 [==============================] - 78s 238us/step - loss: 0.5668 - categorical_accuracy: 0.7837 - val_loss: 0.5636 - val_categorical_accuracy: 0.7838\n",
      "Epoch 3/10\n",
      "328866/328866 [==============================] - 78s 237us/step - loss: 0.5536 - categorical_accuracy: 0.7873 - val_loss: 0.5582 - val_categorical_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "328866/328866 [==============================] - 78s 238us/step - loss: 0.5441 - categorical_accuracy: 0.7902 - val_loss: 0.5577 - val_categorical_accuracy: 0.7880\n",
      "Epoch 5/10\n",
      "328866/328866 [==============================] - 78s 236us/step - loss: 0.5381 - categorical_accuracy: 0.7920 - val_loss: 0.5621 - val_categorical_accuracy: 0.7884\n",
      "Epoch 6/10\n",
      "328866/328866 [==============================] - 78s 236us/step - loss: 0.5324 - categorical_accuracy: 0.7938 - val_loss: 0.5666 - val_categorical_accuracy: 0.7869\n",
      "Epoch 7/10\n",
      "328866/328866 [==============================] - 78s 238us/step - loss: 0.5272 - categorical_accuracy: 0.7955 - val_loss: 0.5680 - val_categorical_accuracy: 0.7829\n",
      "Epoch 8/10\n",
      "328866/328866 [==============================] - 78s 239us/step - loss: 0.5217 - categorical_accuracy: 0.7966 - val_loss: 0.5700 - val_categorical_accuracy: 0.7851\n",
      "Epoch 9/10\n",
      "328866/328866 [==============================] - 79s 239us/step - loss: 0.5171 - categorical_accuracy: 0.7986 - val_loss: 0.5686 - val_categorical_accuracy: 0.7859\n",
      "Epoch 10/10\n",
      "328866/328866 [==============================] - 79s 240us/step - loss: 0.5136 - categorical_accuracy: 0.7996 - val_loss: 0.5697 - val_categorical_accuracy: 0.7861\n",
      "Wall time: 13min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[categorical_accuracy])\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred_classes = encoder.inverse_transform(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.04 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_classes =  encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874108454719717"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_classes, y_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          appearance       0.39      0.60      0.47     10245\n",
      "        aroma/flavor       0.62      0.57      0.59      2551\n",
      "assembly/preparation       0.53      0.57      0.55      3239\n",
      "function/performance       0.92      0.84      0.88     81178\n",
      "              others       0.00      0.00      0.00         0\n",
      "  packaging/labeling       0.45      0.57      0.50      5558\n",
      "\n",
      "           micro avg       0.79      0.79      0.79    102771\n",
      "           macro avg       0.48      0.53      0.50    102771\n",
      "        weighted avg       0.83      0.79      0.80    102771\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred_classes, y_test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable embeddings resulted in worse accuracy than original. In both cases, there is a lot of overfitting. The training loss drop doesn't translate to drop in validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn_model_trainable_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
